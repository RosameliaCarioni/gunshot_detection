{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, Shift, TimeMask, TimeStretch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the elephant listening project \n",
    "general_path = os.path.join('data', 'Clips')\n",
    "\n",
    "# To ensure that both classes have same of samples and to increase the number of gunshots, \n",
    "# I extracted extra data from: https://data.mendeley.com/datasets/x48cwz364j/3 \n",
    "background_path = os.path.join('data', 'Sounds_background')\n",
    "guns_path = os.path.join('data', 'Sounds_gunshots')\n",
    "\n",
    "gunshot_files = [os.path.join(general_path, 'pnnn*'), os.path.join(general_path, 'ecoguns*'), os.path.join(guns_path, '*\\.wav')]\n",
    "\n",
    "no_gunshot_files = [os.path.join(general_path, 'other*'), os.path.join(background_path, '*\\.wav')] \n",
    "gunshot = tf.data.Dataset.list_files(gunshot_files) \n",
    "no_gunshot = tf.data.Dataset.list_files(no_gunshot_files) \n",
    "\n",
    "#to see how many files are in each group: \n",
    "#num_elements = tf.data.experimental.cardinality(no_gunshot).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and return wave "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name): \n",
    "    file_contents = tf.io.read_file(file_name) #retuns a string \n",
    "    wave, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1) # transforms string into actual wav\n",
    "    wave = wave - tf.reduce_mean(wave) # remove the mean \n",
    "    wave = wave / tf.reduce_max(tf.abs(wave)) #normalize \n",
    "    wave = tf.squeeze(wave, axis= -1) #removes axis \n",
    "    #wave = tf.cast(wave * 32768, tf.float32) # value is scaled to look like int16, however, type is kept as float32 for compatibility issues\n",
    "\n",
    "    return wave, sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add labels\n",
    "1: gunshot \n",
    "0: no gunshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunshot = tf.data.Dataset.zip((gunshot, tf.data.Dataset.from_tensor_slices(tf.ones(len(gunshot)))))\n",
    "no_gunshot= tf.data.Dataset.zip((no_gunshot, tf.data.Dataset.from_tensor_slices(tf.zeros(len(gunshot)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(b'data/ollie/ecoguns0.wav', 1.0)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_path = os.path.join('data', 'ollie')\n",
    "gunshot_files = [os.path.join(general_path, 'pnnn*'), os.path.join(general_path, 'ecoguns*')]\n",
    "gunshot = tf.data.Dataset.list_files(gunshot_files) \n",
    "print(len(gunshot))\n",
    "gunshot = tf.data.Dataset.zip((gunshot, tf.data.Dataset.from_tensor_slices(tf.ones(len(gunshot)))))\n",
    "data = gunshot\n",
    "data.as_numpy_iterator().next() # see how it looks like \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concatenate gunshots and no_gunshots into one data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gunshot.concatenate(no_gunshot)\n",
    "data.as_numpy_iterator().next() # see how it looks like \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert data into Spectogram \n",
    "Time frequency compromise: \n",
    "https://www.tensorflow.org/tutorials/audio/simple_audio <br>\n",
    "https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spectogram(wave, label): \n",
    "    max_lenght = 80000 # = 10* 8000, this means 10 seconds \n",
    "\n",
    "    # Padding \n",
    "    wave = wave[:max_lenght] #grab first elements up to max(lengths)\n",
    "    zero_padding = tf.zeros(max_lenght - tf.shape(wave), dtype=tf.float32) # pad with zeros what doesn't meet full length \n",
    "    wave = tf.concat([zero_padding, wave],0) \n",
    "\n",
    "    # Create spectogram \n",
    "    # 1. Fast fourier transform \n",
    "    spectrogram = tf.signal.stft(wave, frame_length=256, frame_step=128)  # Paper: 'Automated detection of gunshots in tropical forests using CNN' \n",
    "    # frame_length =  window length in samples\n",
    "    # frame_step = number of samples to step\n",
    "    # 'Time frequency compromise' \n",
    "    # if window size is small: you get good time resolution in exchange of poor frequency resolution \n",
    "\n",
    "    # 2. Obtain the magnitude of the STFT\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    # 3. Tranform it into appropiate format for deep learning model by adding the channel dimension (in this case 1)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    return spectrogram, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_into_wave(file_path, label): \n",
    "    # Load data\n",
    "    wave, sr = load_data(file_path)\n",
    "    return wave, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/audio-augmentations-in-tensorflow-48483260b169\n",
    "\n",
    "augmentations_pipeline = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeMask(min_band_part=0.0, max_band_part= 0.1, fade = False, p = 0.5), \n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "])  \n",
    "\n",
    "def apply_pipeline(y, sr):\n",
    "    augmented_samples = augmentations_pipeline(samples=y, sample_rate=sr)\n",
    "    #augmented_samples = tf.convert_to_tensor(augmented_samples) #conver it back to a tensor\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def tf_apply_pipeline(feature, label):\n",
    "    \"\"\"\n",
    "    Applies the augmentation pipeline to audio files\n",
    "    @param y: audio data\n",
    "    @param sr: sampling rate\n",
    "    @return: augmented audio data\n",
    "    \"\"\"\n",
    "    augmented_feature = tf.numpy_function(\n",
    "        apply_pipeline, inp=[feature, label], Tout=tf.float32, name=\"apply_pipeline\", stateful = True\n",
    "    )\n",
    "\n",
    "    return augmented_feature, label\n",
    "\n",
    "\n",
    "def augment_audio_dataset(dataset: tf.data.Dataset):\n",
    "    dataset = dataset.map(tf_apply_pipeline)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOW TO USE THE DATA AUGMENTATION METHODS \n",
    "\n",
    "data = data.map(load_file_into_wave) # loading the paths into waves \n",
    "original_data = data \n",
    "data = augment_audio_dataset(data) # create new data \n",
    "data = data.map(make_spectogram) # convert waves into spectograms \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Shuffle the data such that not all gunshots are followed by gunshots, and similarly with no gunshots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess) # calling preprocess method which generates spectograms\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1000) # mixing training samples 1000 at the time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract samples and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = data.as_numpy_iterator()\n",
    "x = []\n",
    "y = []\n",
    "while True:\n",
    "    try: \n",
    "        x_temp, y_temp = iterator.next()\n",
    "        x.append(x_temp)\n",
    "        y.append(y_temp)\n",
    "    except Exception:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 10 \n",
    "kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "from audiomentations import SpecCompose, SpecChannelShuffle, SpecFrequencyMask\n",
    "augment = SpecCompose(\n",
    "    [\n",
    "        SpecFrequencyMask(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "for train, test in kfold.split(x, y):\n",
    "    x_train = np.array(x)[train.astype(int)]\n",
    "    y_train = np.array(y)[train.astype(int)]\n",
    "    x_test = np.array(x)[test.astype(int)]\n",
    "    y_test = np.array(y)[test.astype(int)]\n",
    "\n",
    "    if counter == 0: \n",
    "        #print(x_train)\n",
    "        augmented_spectrogram = augment(x_train)\n",
    "        print('different')\n",
    "        print(augmented_spectrogram)\n",
    "    counter = counter + 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach_thesis_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
