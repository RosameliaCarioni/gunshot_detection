{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, MaxPool2D\n",
    "from tensorflow import keras\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from audiomentations import Compose, AddGaussianNoise, PitchShift, Shift, TimeMask, TimeStretch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from the elephant listening project \n",
    "general_path = os.path.join('data', 'Clips')\n",
    "\n",
    "# To ensure that both classes have same of samples and to increase the number of gunshots, \n",
    "# I extracted extra data from: https://data.mendeley.com/datasets/x48cwz364j/3 \n",
    "background_path = os.path.join('data', 'Sounds_background')\n",
    "guns_path = os.path.join('data', 'Sounds_gunshots')\n",
    "\n",
    "gunshot_files = [os.path.join(general_path, 'pnnn*'), os.path.join(general_path, 'ecoguns*'), os.path.join(guns_path, '*\\.wav')]\n",
    "\n",
    "no_gunshot_files = [os.path.join(general_path, 'other*'), os.path.join(background_path, '*\\.wav')] \n",
    "gunshot = tf.data.Dataset.list_files(gunshot_files) \n",
    "no_gunshot = tf.data.Dataset.list_files(no_gunshot_files) \n",
    "\n",
    "#to see how many files are in each group: \n",
    "#num_elements = tf.data.experimental.cardinality(no_gunshot).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and return wave "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name): \n",
    "    file_contents = tf.io.read_file(file_name) #retuns a string \n",
    "    wave, sample_rate = tf.audio.decode_wav(file_contents, desired_channels=1) # transforms string into actual wav\n",
    "    wave = wave - tf.reduce_mean(wave) # remove the mean \n",
    "    wave = wave / tf.reduce_max(tf.abs(wave)) #normalize \n",
    "    wave = tf.squeeze(wave, axis= -1) #removes axis \n",
    "    #wave = tf.cast(wave * 32768, tf.float32) # value is scaled to look like int16, however, type is kept as float32 for compatibility issues\n",
    "\n",
    "    return wave, sample_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add labels\n",
    "1: gunshot \n",
    "0: no gunshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunshot = tf.data.Dataset.zip((gunshot, tf.data.Dataset.from_tensor_slices(tf.ones(len(gunshot)))))\n",
    "no_gunshot= tf.data.Dataset.zip((no_gunshot, tf.data.Dataset.from_tensor_slices(tf.zeros(len(gunshot)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(b'data/ollie/ecoguns0.wav', 1.0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "general_path = os.path.join('data', 'ollie')\n",
    "gunshot_files = [os.path.join(general_path, 'pnnn*'), os.path.join(general_path, 'ecoguns*')]\n",
    "gunshot = tf.data.Dataset.list_files(gunshot_files) \n",
    "print(len(gunshot))\n",
    "gunshot = tf.data.Dataset.zip((gunshot, tf.data.Dataset.from_tensor_slices(tf.ones(len(gunshot)))))\n",
    "data = gunshot\n",
    "data.as_numpy_iterator().next() # see how it looks like \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Concatenate gunshots and no_gunshots into one data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gunshot.concatenate(no_gunshot)\n",
    "data.as_numpy_iterator().next() # see how it looks like \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert data into Spectogram \n",
    "Time frequency compromise: \n",
    "https://www.tensorflow.org/tutorials/audio/simple_audio <br>\n",
    "https://www.coursera.org/lecture/audio-signal-processing/stft-2-tjEQe \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, TimeMask\n",
    "\n",
    "def preprocess(file_path, label): \n",
    "    # Load data\n",
    "    wave, sr = load_data(file_path)\n",
    "\n",
    "    # TODO: this returns one new sample, whereas I want 4 new ones -- Also, it shouldnt be here most likely\n",
    "    augment = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        TimeMask(min_band_part=0.0, max_band_part= 0.1, fade = False, p = 0.5), \n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    ])  \n",
    "    augmented_samples = augment(samples=wave.numpy(), sample_rate=sr)\n",
    "    augmented_samples = tf.convert_to_tensor(augmented_samples) #conver it back to a tensor\n",
    "    \n",
    "    max_lenght = 80000 # = 10* 8000, this means 10 seconds \n",
    "\n",
    "\n",
    "    # Padding original \n",
    "    wave = wave[:max_lenght] #grab first elements up to max(lengths)\n",
    "    zero_padding = tf.zeros(max_lenght - tf.shape(wave), dtype=tf.float32) # pad with zeros what doesn't meet full length \n",
    "    wave = tf.concat([zero_padding, wave],0) \n",
    "\n",
    "    # Padding augmented \n",
    "    augmented_samples = augmented_samples[:max_lenght] #grab first elements up to max(lengths)\n",
    "    zero_padding = tf.zeros(max_lenght - tf.shape(augmented_samples), dtype=tf.float32) # pad with zeros what doesn't meet full length \n",
    "    augmented_samples = tf.concat([zero_padding, augmented_samples],0) \n",
    "\n",
    "    # Create spectogram \n",
    "    # 1. Fast fourier transform \n",
    "    spectrogram = tf.signal.stft(wave, frame_length=256, frame_step=128)  # Paper: 'Automated detection of gunshots in tropical forests using CNN' \n",
    "    spectrogram2 = tf.signal.stft(augmented_samples, frame_length=256, frame_step=128)\n",
    "\n",
    "    # 2. Obtain the magnitude of the STFT\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "    spectrogram2 = tf.abs(spectrogram2)\n",
    "\n",
    "\n",
    "    # 3. Tranform it into appropiate format for deep learning model by adding the channel dimension (in this case 1)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    spectrogram2 = tf.expand_dims(spectrogram2, axis=2)\n",
    "\n",
    "    return spectrogram, label, spectrogram2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path, label): \n",
    "    # Load data\n",
    "    wave, sr = load_data(file_path)\n",
    "    max_lenght = 80000 # = 10* 8000, this means 10 seconds \n",
    "\n",
    "    # Padding \n",
    "    wave = wave[:max_lenght] #grab first elements up to max(lengths)\n",
    "    zero_padding = tf.zeros(max_lenght - tf.shape(wave), dtype=tf.float32) # pad with zeros what doesn't meet full length \n",
    "    wave = tf.concat([zero_padding, wave],0) \n",
    "\n",
    "    # Create spectogram \n",
    "    # 1. Fast fourier transform \n",
    "    spectrogram = tf.signal.stft(wave, frame_length=256, frame_step=128)  # Paper: 'Automated detection of gunshots in tropical forests using CNN' \n",
    "    # frame_length =  window length in samples\n",
    "    # frame_step = number of samples to step\n",
    "    # 'Time frequency compromise' \n",
    "    # if window size is small: you get good time resolution in exchange of poor frequency resolution \n",
    "\n",
    "    # 2. Obtain the magnitude of the STFT\n",
    "    spectrogram = tf.abs(spectrogram)\n",
    "\n",
    "    # 3. Tranform it into appropiate format for deep learning model by adding the channel dimension (in this case 1)\n",
    "    spectrogram = tf.expand_dims(spectrogram, axis=2)\n",
    "    return spectrogram, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/audio-augmentations-in-tensorflow-48483260b169\n",
    "\n",
    "augmentations_pipeline = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "    TimeMask(min_band_part=0.0, max_band_part= 0.1, fade = False, p = 0.5), \n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "])  \n",
    "\n",
    "def apply_pipeline(y, sr):\n",
    "    augmented_samples = augmentations_pipeline(samples=y, sample_rate=sr)\n",
    "    augmented_samples = tf.convert_to_tensor(augmented_samples) #conver it back to a tensor\n",
    "    \n",
    "    return augmented_samples\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def tf_apply_pipeline(feature, label):\n",
    "    \"\"\"\n",
    "    Applies the augmentation pipeline to audio files\n",
    "    @param y: audio data\n",
    "    @param sr: sampling rate\n",
    "    @return: augmented audio data\n",
    "    \"\"\"\n",
    "    augmented_feature = tf.numpy_function(\n",
    "        apply_pipeline, inp=[feature, label], Tout=tf.float32, name=\"apply_pipeline\"\n",
    "    )\n",
    "\n",
    "    return augmented_feature, label\n",
    "\n",
    "\n",
    "def augment_audio_dataset(dataset: tf.data.Dataset):\n",
    "    dataset = dataset.map(tf_apply_pipeline)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pyimagesearch.com/2021/06/28/data-augmentation-with-tf-data-and-tensorflow/\n",
    "def augment_using_ops(file_path, labels):\n",
    "    wave, sample_rate = load_data(file_path)\n",
    "\n",
    "    augment = Compose([\n",
    "        AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=0.5),\n",
    "        TimeMask(min_band_part=0.0, max_band_part= 0.1, fade = False, p = 0.5), \n",
    "        PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "        TimeStretch(min_rate=0.8, max_rate=1.25, p=0.5),\n",
    "    ])  \n",
    "\n",
    "    augmented_samples = augment(samples=wave, sample_rate= sample_rate)\n",
    "  \n",
    "    print('hello2')\n",
    "    augmented_samples = tf.convert_to_tensor(augmented_samples) #conver it back to a tensor\n",
    "    return augmented_samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/ipykernel_87546/81929746.py\", line 12, in augment_using_ops  *\n        augmented_samples = augment(samples=wave, sample_rate= sample_rate)\n    File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/composition.py\", line 90, in __call__  *\n        samples = transform(samples, sample_rate)\n    File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/transforms_interface.py\", line 84, in __call__  *\n        return self.apply(samples, sample_rate)\n    File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/augmentations/add_gaussian_noise.py\", line 35, in apply  *\n        noise = np.random.randn(*samples.shape).astype(np.float32)\n    File \"mtrand.pyx\", line 1246, in numpy.random.mtrand.RandomState.randn  **\n        \n    File \"mtrand.pyx\", line 1403, in numpy.random.mtrand.RandomState.standard_normal\n        \n    File \"_common.pyx\", line 630, in numpy.random._common.cont\n        \n\n    TypeError: 'NoneType' object cannot be interpreted as an integer\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mmap(augment_using_ops)\n\u001b[1;32m      2\u001b[0m data\u001b[39m.\u001b[39mas_numpy_iterator()\u001b[39m.\u001b[39mnext()\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:2240\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2236\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2237\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2238\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2239\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2240\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39;49m_map_v2(\n\u001b[1;32m   2241\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2242\u001b[0m     map_func,\n\u001b[1;32m   2243\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[1;32m   2244\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m   2245\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m debug_mode\u001b[39m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/map_op.py:107\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality \u001b[39m=\u001b[39m preserve_cardinality\n\u001b[0;32m--> 107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[1;32m    108\u001b[0m     map_func,\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[1;32m    110\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[1;32m    111\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[1;32m    113\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[39m.\u001b[39m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_common_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:261\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    255\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    256\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    257\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    258\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[0;32m--> 261\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[1;32m    262\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[1;32m    263\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:232\u001b[0m, in \u001b[0;36mTracingCompiler.get_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    224\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Returns a `ConcreteFunction` specialized to inputs and execution context.\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \n\u001b[1;32m    226\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39m      `tf.Tensor` or `tf.TensorSpec`.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m   concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\n\u001b[1;32m    233\u001b[0m       \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    234\u001b[0m   concrete_function\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    235\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:202\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mmake_canonicalized_monomorphic_type(args, kwargs)\n\u001b[1;32m    201\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 202\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    203\u001b[0m   seen_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    204\u001b[0m   concrete_function\u001b[39m.\u001b[39m_arg_keywords \u001b[39m=\u001b[39m []  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:166\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    164\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:396\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m   args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[1;32m    394\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m--> 396\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[1;32m    397\u001b[0m     args, kwargs, func_graph)\n\u001b[1;32m    399\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    400\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39m_function_captures  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:300\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[1;32m    299\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 300\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    301\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    302\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    303\u001b[0m         args,\n\u001b[1;32m    304\u001b[0m         kwargs,\n\u001b[1;32m    305\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    306\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    307\u001b[0m         autograph\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph,\n\u001b[1;32m    308\u001b[0m         autograph_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_autograph_options,\n\u001b[1;32m    309\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    310\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[1;32m    311\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    313\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    314\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    315\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    319\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1214\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1212\u001b[0m   _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1214\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1216\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1218\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:238\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39m@eager_function\u001b[39m\u001b[39m.\u001b[39mdefun_with_attributes(\n\u001b[1;32m    233\u001b[0m     input_signature\u001b[39m=\u001b[39mstructure\u001b[39m.\u001b[39mget_flat_tensor_specs(\n\u001b[1;32m    234\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_structure),\n\u001b[1;32m    235\u001b[0m     autograph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    236\u001b[0m     attributes\u001b[39m=\u001b[39mdefun_kwargs)\n\u001b[1;32m    237\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    239\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[1;32m    240\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/structured_function.py:169\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[1;32m    168\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[0;32m--> 169\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[1;32m    170\u001b[0m ret \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:692\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 692\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mag_error_metadata\u001b[39m.\u001b[39mto_exception(e)\n\u001b[1;32m    693\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    694\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/__autograph_generated_fileljzs3osu.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__augment_using_ops\u001b[0;34m(file_path, labels)\u001b[0m\n\u001b[1;32m     10\u001b[0m (wave, sample_rate) \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(load_data), (ag__\u001b[39m.\u001b[39mld(file_path),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m augment \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(Compose), ([ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(AddGaussianNoise), (), \u001b[39mdict\u001b[39m(min_amplitude\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m, max_amplitude\u001b[39m=\u001b[39m\u001b[39m0.015\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m), fscope), ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(TimeMask), (), \u001b[39mdict\u001b[39m(min_band_part\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m, max_band_part\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, fade\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m), fscope), ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(PitchShift), (), \u001b[39mdict\u001b[39m(min_semitones\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m, max_semitones\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m), fscope), ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(TimeStretch), (), \u001b[39mdict\u001b[39m(min_rate\u001b[39m=\u001b[39m\u001b[39m0.8\u001b[39m, max_rate\u001b[39m=\u001b[39m\u001b[39m1.25\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m), fscope)],), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 12\u001b[0m augmented_samples \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(augment), (), \u001b[39mdict\u001b[39;49m(samples\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(wave), sample_rate\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mld(sample_rate)), fscope)\n\u001b[1;32m     13\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mhello2\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m augmented_samples \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mconvert_to_tensor, (ag__\u001b[39m.\u001b[39mld(augmented_samples),), \u001b[39mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/__autograph_generated_filez1175n91.py:54\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, samples, sample_rate)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m     53\u001b[0m transform \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mld(should_apply), if_body_1, else_body_1, get_state_2, set_state_2, (\u001b[39m'\u001b[39;49m\u001b[39msamples\u001b[39;49m\u001b[39m'\u001b[39;49m,), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     55\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py:1266\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1264\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1266\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py:1319\u001b[0m, in \u001b[0;36m_py_if_stmt\u001b[0;34m(cond, body, orelse)\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_py_if_stmt\u001b[39m(cond, body, orelse):\n\u001b[1;32m   1318\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Overload of if_stmt that executes a Python if statement.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1319\u001b[0m   \u001b[39mreturn\u001b[39;00m body() \u001b[39mif\u001b[39;00m cond \u001b[39melse\u001b[39;00m orelse()\n",
      "File \u001b[0;32m/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/__autograph_generated_filez1175n91.py:48\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.if_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     samples \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(transform), (ag__\u001b[39m.\u001b[39mld(samples), ag__\u001b[39m.\u001b[39mld(sample_rate)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     47\u001b[0m transform \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mtransform\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m ag__\u001b[39m.\u001b[39;49mfor_stmt(ag__\u001b[39m.\u001b[39;49mld(transforms), \u001b[39mNone\u001b[39;49;00m, loop_body, get_state_1, set_state_1, (\u001b[39m'\u001b[39;49m\u001b[39msamples\u001b[39;49m\u001b[39m'\u001b[39;49m,), {\u001b[39m'\u001b[39;49m\u001b[39miterate_names\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mtransform\u001b[39;49m\u001b[39m'\u001b[39;49m})\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py:451\u001b[0m, in \u001b[0;36mfor_stmt\u001b[0;34m(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(iter_, distribute\u001b[39m.\u001b[39mIterable):\n\u001b[1;32m    448\u001b[0m   \u001b[39m# TODO(b/162250181): Use _tf_iterator_for_stmt(iter(iter_)...\u001b[39;00m\n\u001b[1;32m    449\u001b[0m   for_fn \u001b[39m=\u001b[39m _tf_distributed_iterable_for_stmt\n\u001b[0;32m--> 451\u001b[0m for_fn(iter_, extra_test, body, get_state, set_state, symbol_names, opts)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py:502\u001b[0m, in \u001b[0;36m_py_for_stmt\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m   \u001b[39mfor\u001b[39;00m target \u001b[39min\u001b[39;00m iter_:\n\u001b[0;32m--> 502\u001b[0m     body(target)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py:468\u001b[0m, in \u001b[0;36m_py_for_stmt.<locals>.protected_body\u001b[0;34m(protected_iter)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprotected_body\u001b[39m(protected_iter):\n\u001b[0;32m--> 468\u001b[0m   original_body(protected_iter)\n\u001b[1;32m    469\u001b[0m   after_iteration()\n\u001b[1;32m    470\u001b[0m   before_iteration()\n",
      "File \u001b[0;32m/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/__autograph_generated_filez1175n91.py:46\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.if_body_1.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mnonlocal\u001b[39;00m samples\n\u001b[1;32m     45\u001b[0m transform \u001b[39m=\u001b[39m itr\n\u001b[0;32m---> 46\u001b[0m samples \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(transform), (ag__\u001b[39m.\u001b[39;49mld(samples), ag__\u001b[39m.\u001b[39;49mld(sample_rate)), \u001b[39mNone\u001b[39;49;00m, fscope)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:441\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args)\n\u001b[1;32m    442\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m   _attach_error_metadata(e, converted_f)\n",
      "File \u001b[0;32m/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/__autograph_generated_fileu1_lptyj.py:115\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__\u001b[0;34m(self, samples, sample_rate)\u001b[0m\n\u001b[1;32m    113\u001b[0m         do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    114\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mand_(\u001b[39mlambda\u001b[39;49;00m : ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mparameters[\u001b[39m'\u001b[39;49m\u001b[39mshould_apply\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mlambda\u001b[39;49;00m : ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mlen\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(samples),), \u001b[39mNone\u001b[39;49;00m, fscope) \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m), if_body_6, else_body_6, get_state_6, set_state_6, (\u001b[39m'\u001b[39;49m\u001b[39mdo_return\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mretval_\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m2\u001b[39;49m)\n\u001b[1;32m    116\u001b[0m \u001b[39mreturn\u001b[39;00m fscope\u001b[39m.\u001b[39mret(retval_, do_return)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py:1264\u001b[0m, in \u001b[0;36mif_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[39m# Note: tf.cond doesn't support SparseTensor.\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m \u001b[39mif\u001b[39;00m tensors\u001b[39m.\u001b[39mis_dense_tensor(cond):\n\u001b[0;32m-> 1264\u001b[0m   _tf_if_stmt(cond, body, orelse, get_state, set_state, symbol_names, nouts)\n\u001b[1;32m   1265\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1266\u001b[0m   _py_if_stmt(cond, body, orelse)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/operators/control_flow.py:1310\u001b[0m, in \u001b[0;36m_tf_if_stmt\u001b[0;34m(cond, body, orelse, get_state, set_state, symbol_names, nouts)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     _verify_tf_cond_vars(new_body_vars_[\u001b[39m0\u001b[39m], new_orelse_vars, symbol_names)\n\u001b[1;32m   1308\u001b[0m   \u001b[39mreturn\u001b[39;00m new_orelse_vars\n\u001b[0;32m-> 1310\u001b[0m final_cond_vars \u001b[39m=\u001b[39m control_flow_ops\u001b[39m.\u001b[39;49mcond(\n\u001b[1;32m   1311\u001b[0m     cond, aug_body, aug_orelse, strict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1312\u001b[0m final_cond_vars \u001b[39m=\u001b[39m final_cond_vars \u001b[39m+\u001b[39m init_vars[nouts:]\n\u001b[1;32m   1314\u001b[0m set_state(final_cond_vars)\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/__autograph_generated_fileu1_lptyj.py:102\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf____call__.<locals>.if_body_6\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mapply, (ag__\u001b[39m.\u001b[39mld(samples), ag__\u001b[39m.\u001b[39mld(sample_rate)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m    103\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/__autograph_generated_file7n29tmla.py:10\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__apply\u001b[0;34m(self, samples, sample_rate)\u001b[0m\n\u001b[1;32m      8\u001b[0m do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      9\u001b[0m retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 10\u001b[0m noise \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(np)\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn, \u001b[39mtuple\u001b[39m(ag__\u001b[39m.\u001b[39mld(samples)\u001b[39m.\u001b[39mshape), \u001b[39mNone\u001b[39;00m, fscope)\u001b[39m.\u001b[39mastype, (ag__\u001b[39m.\u001b[39mld(np)\u001b[39m.\u001b[39mfloat32,), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     11\u001b[0m samples \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mld(samples) \u001b[39m+\u001b[39m ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mparameters[\u001b[39m'\u001b[39m\u001b[39mamplitude\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m ag__\u001b[39m.\u001b[39mld(noise)\n\u001b[1;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32mmtrand.pyx:1246\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mmtrand.pyx:1403\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:630\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/ipykernel_87546/81929746.py\", line 12, in augment_using_ops  *\n        augmented_samples = augment(samples=wave, sample_rate= sample_rate)\n    File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/composition.py\", line 90, in __call__  *\n        samples = transform(samples, sample_rate)\n    File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/transforms_interface.py\", line 84, in __call__  *\n        return self.apply(samples, sample_rate)\n    File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/augmentations/add_gaussian_noise.py\", line 35, in apply  *\n        noise = np.random.randn(*samples.shape).astype(np.float32)\n    File \"mtrand.pyx\", line 1246, in numpy.random.mtrand.RandomState.randn  **\n        \n    File \"mtrand.pyx\", line 1403, in numpy.random.mtrand.RandomState.standard_normal\n        \n    File \"_common.pyx\", line 630, in numpy.random._common.cont\n        \n\n    TypeError: 'NoneType' object cannot be interpreted as an integer\n"
     ]
    }
   ],
   "source": [
    "data = data.map(augment_using_ops)\n",
    "data.as_numpy_iterator().next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = augment_audio_dataset(data)\n",
    "#data = data.map(lambda y, sr: (tf.expand_dims(y, axis=-1), sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-04 16:50:25.017903: W tensorflow/core/framework/op_kernel.cc:1818] UNKNOWN: AttributeError: 'bytes' object has no attribute 'dtype'\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n",
      "    ret = func(*args)\n",
      "\n",
      "  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "\n",
      "  File \"/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/ipykernel_87546/2627490433.py\", line 11, in apply_pipeline\n",
      "    augmented_samples = augmentations_pipeline(samples=y, sample_rate=sr)\n",
      "\n",
      "  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/composition.py\", line 90, in __call__\n",
      "    samples = transform(samples, sample_rate)\n",
      "\n",
      "  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/transforms_interface.py\", line 56, in __call__\n",
      "    if samples.dtype == np.float64:\n",
      "\n",
      "AttributeError: 'bytes' object has no attribute 'dtype'\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} AttributeError: 'bytes' object has no attribute 'dtype'\nTraceback (most recent call last):\n\n  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/ipykernel_87546/2627490433.py\", line 11, in apply_pipeline\n    augmented_samples = augmentations_pipeline(samples=y, sample_rate=sr)\n\n  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/composition.py\", line 90, in __call__\n    samples = transform(samples, sample_rate)\n\n  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/transforms_interface.py\", line 56, in __call__\n    if samples.dtype == np.float64:\n\nAttributeError: 'bytes' object has no attribute 'dtype'\n\n\n\t [[{{function_node __inference_tf_apply_pipeline_150}}{{node apply_pipeline}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data\u001b[39m.\u001b[39;49mas_numpy_iterator()\u001b[39m.\u001b[39;49mnext()\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4665\u001b[0m, in \u001b[0;36m_NumpyIterator.next\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4664\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m-> 4665\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__next__\u001b[39;49m()\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py:4662\u001b[0m, in \u001b[0;36m_NumpyIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4659\u001b[0m     numpy\u001b[39m.\u001b[39msetflags(write\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   4660\u001b[0m   \u001b[39mreturn\u001b[39;00m numpy\n\u001b[0;32m-> 4662\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mmap_structure(to_numpy, \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator))\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:797\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    796\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 797\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    798\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    799\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:780\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    779\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 780\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    781\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    782\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    783\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    785\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3014\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3015\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 3016\u001b[0m   _ops\u001b[39m.\u001b[39;49mraise_from_not_ok_status(e, name)\n\u001b[1;32m   3017\u001b[0m \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_FallbackException:\n\u001b[1;32m   3018\u001b[0m   \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7262\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7261\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7262\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mUnknownError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} AttributeError: 'bytes' object has no attribute 'dtype'\nTraceback (most recent call last):\n\n  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/var/folders/bz/rfsj8qqx5zsg0g1p0xdb1fkw0000gn/T/ipykernel_87546/2627490433.py\", line 11, in apply_pipeline\n    augmented_samples = augmentations_pipeline(samples=y, sample_rate=sr)\n\n  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/composition.py\", line 90, in __call__\n    samples = transform(samples, sample_rate)\n\n  File \"/Users/rosameliacarioni/miniconda3/envs/bach_thesis_4/lib/python3.9/site-packages/audiomentations/core/transforms_interface.py\", line 56, in __call__\n    if samples.dtype == np.float64:\n\nAttributeError: 'bytes' object has no attribute 'dtype'\n\n\n\t [[{{function_node __inference_tf_apply_pipeline_150}}{{node apply_pipeline}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.map_op._MapDataset"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.map(preprocess) # calling preprocess method which generates spectograms\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       " \n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       " \n",
       "        [[ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         ...,\n",
       "         [ 0.        ],\n",
       "         [ 0.        ],\n",
       "         [ 0.        ]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[12.588182  ],\n",
       "         [10.061134  ],\n",
       "         [ 6.052571  ],\n",
       "         ...,\n",
       "         [ 0.20084883],\n",
       "         [ 0.2561689 ],\n",
       "         [ 0.22029305]],\n",
       " \n",
       "        [[ 7.165429  ],\n",
       "         [ 2.824443  ],\n",
       "         [ 3.6046548 ],\n",
       "         ...,\n",
       "         [ 0.273996  ],\n",
       "         [ 0.23777206],\n",
       "         [ 0.05448246]],\n",
       " \n",
       "        [[13.210068  ],\n",
       "         [10.965674  ],\n",
       "         [ 7.341511  ],\n",
       "         ...,\n",
       "         [ 0.12540467],\n",
       "         [ 0.11350828],\n",
       "         [ 0.20571661]]], dtype=float32),\n",
       " 1.0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original \n",
    "file_name = 'data/Clips/ecoguns0.wav' # seems good  \n",
    "spectrogram, label, spectogram2  = preprocess(file_name, '1')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectogram2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'data/Clips/ecoguns0.wav' # seems good  \n",
    "spectrogram, label = preprocess(file_name, '1')\n",
    "spectrogram\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Shuffle the data such that not all gunshots are followed by gunshots, and similarly with no gunshots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.map(preprocess) # calling preprocess method which generates spectograms\n",
    "data = data.cache()\n",
    "data = data.shuffle(buffer_size=1000) # mixing training samples 1000 at the time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract samples and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = data.as_numpy_iterator()\n",
    "x = []\n",
    "y = []\n",
    "while True:\n",
    "    try: \n",
    "        x_temp, y_temp = iterator.next()\n",
    "        x.append(x_temp)\n",
    "        y.append(y_temp)\n",
    "    except Exception:\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 10 \n",
    "kfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "from audiomentations import SpecCompose, SpecChannelShuffle, SpecFrequencyMask\n",
    "augment = SpecCompose(\n",
    "    [\n",
    "        SpecFrequencyMask(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "for train, test in kfold.split(x, y):\n",
    "    x_train = np.array(x)[train.astype(int)]\n",
    "    y_train = np.array(y)[train.astype(int)]\n",
    "    x_test = np.array(x)[test.astype(int)]\n",
    "    y_test = np.array(y)[test.astype(int)]\n",
    "\n",
    "    if counter == 0: \n",
    "        #print(x_train)\n",
    "        augmented_spectrogram = augment(x_train)\n",
    "        print('different')\n",
    "        print(augmented_spectrogram)\n",
    "    counter = counter + 1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bach_thesis_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
